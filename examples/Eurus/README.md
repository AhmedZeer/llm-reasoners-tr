# Eurus

This is an example of using Eurus-LM to perform best-of-N sampling with Llama-3 8B as the base model.

Prerequisites:
- Download LLama-3 8B model.
- Have 2 * 24 GB GPUs.

Script:
```bash
CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.run --nproc_per_node 1 examples/Eurus/inference.py --model_dir $LLAMA3_CKPTS --best_of_n 10
```

**Results**:

We tested the performance of using Eurus-RM-7B to select the best of 10 reasoning chains generated by Llama-2 8B, on the first 100 samples of GSM8k testset.

|Method|Accuracy|
|-|-|
|CoT (Llama-8B)|0.54|
|CoT (Llama-8B) +Best-of-10 (Eurus-RM-7B) | 0.74|
 

## Reference
```bibtex
@article{yuan2024advancing,
  title={Advancing LLM Reasoning Generalists with Preference Trees},
  author={Yuan, Lifan and Cui, Ganqu and Wang, Hanbin and Ding, Ning and Wang, Xingyao and Deng, Jia and Shan, Boji and Chen, Huimin and Xie, Ruobing and Lin, Yankai and others},
  journal={arXiv preprint arXiv:2404.02078},
  year={2024}
}
```