# AutoRace

This document will guide you to use the autorace to evaluate your reasoning chains (reproduce the experiments or apply it to your own datasets)

## Set Up

`cd autorace` and set the OPENAI_API_KEY:

```
export OPENAI_API_KEY=YOUR_OWN_OPEN_AI_KEY
```

Or you can input it to terminal when running "autorace.py"

## Reproduce the evaluation accuracy results in paper (Table 1)

```
python autorace.py --reproduce_tab1
```

## Evaluate your own result

### Format your results

The output of your `REASONING_MODEL` on `DATASET` should be a .jsonlines file in the following format:

```jsonl
{
  "question": question in original dataset
  "reasoning_chain": a rationale generated by your reasoning model
  "answer": the ground truth answer of the question in the dataset
}
```

The path of this file should be `autorace/data/{REASONING_MODEL}/{DATASET}.jsonl`

### Criteria Generation
Theoretically, autorace can support any evaluation of Chain-of-Thought. 

Currently, we have supported:

 `dataset_list=['gsm8k', 'strategyqa', 'AQuA', 'cosmos', 'multistep_arithmetic', 'word_sorting', 'logical_deduction']`
 
* If you want to evaluate your reasoning model on other datasets, you should first generate the criteria:
  ```python
  python autorace.py --gen_criteria --dataset="YOUR_DATASET" --criteria_path="YOUR_CRITERIA_PROMPT_PATH.txt"
  ``` 
    
    Generated criterias will be written into `prompt.json`
  2. fill in `PROMPT_TYPE_DICT` in `autorace.py` accordingly

* **If you are using the dataset already available in `dataset list`, but testing a different reasoning model, you don't need to run `autorace_criterion()` to generate a criterion prompt.** You should use criterion prompt corresponding to the dataset in `prompt.json`.

### Get AutoRace Score

Then, run `python autorace.py --dataset="DATASET" --reasoning_model="REASONING_MODEL" --output_log="OUTPUT_PATH"` for evaluation. The AutoRace results will be under 
`OUTPUT_PATH`. By default, `OUTPUT_PATH` is `log/auto_race`




